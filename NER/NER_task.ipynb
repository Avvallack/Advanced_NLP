{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://dl.dropboxusercontent.com/s/tlijezgr8tnpeym/ner_dataset.csv?dl=0', \n",
    "                 header=0, \n",
    "                 encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentence #'].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(y=df['Tag']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sen = df['Sentence #'].nunique()\n",
    "print(f'Number of sentences = {num_sen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_words = df['Sentence #'].value_counts().mean()\n",
    "min_words = df['Sentence #'].value_counts().min()\n",
    "max_words = df['Sentence #'].value_counts().max()\n",
    "print('Mean number of tokems in sentence= {:.3f}'.format(mean_words))\n",
    "print('Min number of tokems in sentence= {}'.format(min_words))\n",
    "print('Max number of tokems in sentence= {}'.format(max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt_words = Counter(df['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The size of voxcabulary = {}'.format(len(cnt_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(by='Sentence #').agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = grouped.iloc[-5000:,[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = grouped.iloc[ :-5000, [0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.unique(np.array([word for words in train_dataset['Word'].values for word in words])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = df['Tag'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hmm = [[(word, tag) for tag, word in zip(*element)] for element in zip(train_dataset['Word'], train_dataset['Tag'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hmm = test_dataset['Word'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HMM import HMMTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = HMMTagger(states, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.fit(train_hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = tagger.predict(test_hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [tag for tags in test_dataset['Tag'] for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [tag for tags in predict for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_true, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_corpus = [[(word, pos, tag) for word, pos, tag in zip(*element)] \n",
    "              for element in zip(grouped['Word'], grouped['POS'], grouped['Tag'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in crf_corpus]\n",
    "y = [sent2labels(s) for s in crf_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(algorithm='lbfgs',\n",
    "c1=10,\n",
    "c2=0.1,\n",
    "max_iterations=100,\n",
    "all_possible_transitions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BiDirectionalLSTM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = grouped['Word'].values\n",
    "tags = grouped['Tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NerDataSet(sentences, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 2\n",
    "device = 'cpu'\n",
    "labels = list(range(1,18))\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_set, test_set = random_split(ds, [40000, 7959], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = padded_data_loader(data=train_set, workers=workers, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = padded_data_loader(data=test_set, workers=workers, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(len(ds.vocab), len(ds.tag_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "conf_matrix = np.zeros((len(ds.tag_vocab)-1, len(ds.tag_vocab)-1))\n",
    "total_step = len(train_loader)\n",
    "scores = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sentences, tags) in enumerate(train_loader):\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sentences)\n",
    "        loss = F.cross_entropy(torch.flatten(outputs, 0, 1), torch.flatten(tags, 0, 1), ignore_index=0)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for  sentences, tags in test_loader:\n",
    "            sentences = sentences.to(device)\n",
    "            tags = tags.to(device)\n",
    "            tags = tags.flatten()\n",
    "            tag_mask = tags != 0\n",
    "            outputs = model(sentences)\n",
    "            predicted = outputs.argmax(2)\n",
    "            tags = tags[tag_mask]\n",
    "            predicted = predicted.flatten()[tag_mask]\n",
    "            conf_matrix += confusion_matrix(tags.numpy(), predicted.numpy(), labels=labels)\n",
    "        tp = np.diagonal(conf_matrix)\n",
    "        prec = tp / conf_matrix.sum(axis=0)\n",
    "        rec = tp / conf_matrix.sum(axis=1)\n",
    "        mask = np.logical_and(prec == 0, rec == 0)\n",
    "        f1 = 2 * (prec * rec /(prec + rec))\n",
    "        f1[mask] = 0\n",
    "        print('Macro avg for f1 score on {} epoch = {:.3f}'.format(epoch, f1.mean()))\n",
    "        scores.append(f1.mean())\n",
    "    model.train()\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "model = NerNN(len(ds.vocab), len(ds.tag_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=30, min_epochs=5 )\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "conf_matrix = np.zeros((len(ds.tag_vocab)-1, len(ds.tag_vocab)-1))\n",
    "for  sentences, tags in test_loader:\n",
    "    tags = tags.flatten()\n",
    "    tag_mask = tags != 0\n",
    "    outputs = model(sentences)\n",
    "    predicted = outputs.argmax(2)\n",
    "    tags = tags[tag_mask]\n",
    "    predicted = predicted.flatten()[tag_mask]\n",
    "    conf_matrix += confusion_matrix(tags.numpy(), predicted.numpy(), labels=labels)\n",
    "tp = np.diagonal(conf_matrix)\n",
    "prec = tp / conf_matrix.sum(axis=0)\n",
    "rec = tp / conf_matrix.sum(axis=1)\n",
    "mask = np.logical_and(prec == 0, rec == 0)\n",
    "f1 = 2 * (prec * rec /(prec + rec))\n",
    "f1[mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I-eve</th>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-org</th>\n",
       "      <td>0.635064</td>\n",
       "      <td>0.729313</td>\n",
       "      <td>0.678933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-nat</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-art</th>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.183673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-gpe</th>\n",
       "      <td>0.936557</td>\n",
       "      <td>0.928654</td>\n",
       "      <td>0.932589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.990480</td>\n",
       "      <td>0.982101</td>\n",
       "      <td>0.986273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-gpe</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.711864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-per</th>\n",
       "      <td>0.831557</td>\n",
       "      <td>0.858086</td>\n",
       "      <td>0.844613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-tim</th>\n",
       "      <td>0.766473</td>\n",
       "      <td>0.826541</td>\n",
       "      <td>0.795375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-org</th>\n",
       "      <td>0.623873</td>\n",
       "      <td>0.749242</td>\n",
       "      <td>0.680834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-eve</th>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.281690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-geo</th>\n",
       "      <td>0.858507</td>\n",
       "      <td>0.837624</td>\n",
       "      <td>0.847937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-art</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.037736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-nat</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-per</th>\n",
       "      <td>0.778523</td>\n",
       "      <td>0.804673</td>\n",
       "      <td>0.791382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-tim</th>\n",
       "      <td>0.876365</td>\n",
       "      <td>0.912442</td>\n",
       "      <td>0.894040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-geo</th>\n",
       "      <td>0.741987</td>\n",
       "      <td>0.790777</td>\n",
       "      <td>0.765606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          recall  precision        f1\n",
       "labels                               \n",
       "I-eve   0.233333   0.269231  0.250000\n",
       "B-org   0.635064   0.729313  0.678933\n",
       "B-nat   0.333333   0.500000  0.400000\n",
       "B-art   0.147541   0.243243  0.183673\n",
       "B-gpe   0.936557   0.928654  0.932589\n",
       "O       0.990480   0.982101  0.986273\n",
       "I-gpe   0.700000   0.724138  0.711864\n",
       "I-per   0.831557   0.858086  0.844613\n",
       "I-tim   0.766473   0.826541  0.795375\n",
       "I-org   0.623873   0.749242  0.680834\n",
       "B-eve   0.277778   0.285714  0.281690\n",
       "B-geo   0.858507   0.837624  0.847937\n",
       "I-art   0.031250   0.047619  0.037736\n",
       "I-nat   0.153846   0.666667  0.250000\n",
       "B-per   0.778523   0.804673  0.791382\n",
       "B-tim   0.876365   0.912442  0.894040\n",
       "I-geo   0.741987   0.790777  0.765606"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(ds.tag_vocab.keys())[1:]\n",
    "report = pd.DataFrame.from_dict({'labels': labels, 'recall': rec, 'precision': prec, 'f1': f1})\n",
    "report.set_index('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6077967853585879, 0.583321623993766, 0.6562391965765241)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.mean(), rec.mean(), prec.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
